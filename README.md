# Twitter Scraper

A Python-based Twitter scraper that uses dynamic account management and stores data in MySQL.

## Features

- **Dynamic Account Management**: Automatically fetches Twitter accounts from database
- **Error Handling**: Marks failed accounts as "error" with detailed logging
- **Data Storage**: Stores tweets and profiles in MySQL database
- **Account Locking**: Prevents multiple scrapers from using the same account

## Project Structure

```
scrapers/
├── scraper.py                    # Main scraper script
├── constants.py                  # SQL queries
├── db_configs.py                 # Database configuration
├── utils/
│   ├── twitter_utils.py          # Twitter data processing
│   └── scraper_account_utils.py  # Account management
└── accounts.db                   # SQLite file (auto-generated by twscrape)
```

## Database Tables

### `configs.twitter_scrapers`
Stores Twitter account credentials and status:
- `username` - Twitter username
- `cookie` - Authentication cookies
- `status` - Account status (active/error)
- `is_occupied` - Lock status
- `error_message` - Error details if status = 'error'

### `twitter.enhanced_tweets`
Stores scraped tweet data with metadata.

### `twitter.twitter_profiles`
Stores user profile information.

## Setup

1. **Environment Variables** (`.env` file):
   ```
   DB_HOST=your_host
   DB_USER=your_user
   DB_PASSWORD=your_password
   DB_NAME=your_database
   DB_PORT=3306
   ```

2. **Install Dependencies**:
   ```bash
   pip install twscrape pymysql python-dotenv
   ```

3. **Database Setup**:
   - Ensure `configs.twitter_scrapers` table exists
   - Add active Twitter accounts with valid cookies

## Usage

```bash
python scraper.py
```

## Account Management Flow

1. **Fetch Account**: Gets random available account from database
2. **Lock Account**: Marks as occupied to prevent conflicts
3. **Scrape Data**: Uses account to scrape Twitter data
4. **Handle Results**:
   - Success → Mark account as available
   - Error → Mark account as "error" with exception details

## Error Handling

- Failed accounts are marked with `status = 'error'`
- Exception details stored in `error_message` field
- Error accounts are excluded from future selection
- Manual intervention required to reactivate error accounts

## Configuration

- **Default Account**: Fallback credentials in `scraper.py`
- **Scraping Target**: Currently set to scrape "ostrich_hq"
- **Time Range**: Last 2 days of tweets
- **Limit**: 100 tweets per run

## Notes

- `accounts.db` is auto-generated by twscrape library
- All SQL queries centralized in `constants.py`
- Account locking prevents concurrent usage
- Error accounts need manual status reset to 'active'
